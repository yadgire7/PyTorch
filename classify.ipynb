{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image as mp_image\n",
    "import seaborn as sns\n",
    "\n",
    "# Required magic to display matplotlib plots in notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Compost', 'Landfill', 'Recycle']\n"
     ]
    }
   ],
   "source": [
    "training_folder_name = 'C:/Users/admin/Desktop/Final Project/train' #foldername should be train\n",
    "validation_folder_name = 'C:/Users/admin/Desktop/Final Project/val'\n",
    "# All images are 128x128 pixels\n",
    "img_size = (128, 128)\n",
    "\n",
    "# The folder contains a subfolder for each class of shape\n",
    "classes = sorted(os.listdir(training_folder_name))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported - ready to use PyTorch 1.10.2+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# function to resize image\n",
    "def resize_image(src_image, size=(224, 224), bg_color=\"white\"): \n",
    "    from PIL import Image, ImageOps, ImageFile\n",
    "    ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "    # resize the image so the longest dimension matches our target size\n",
    "    src_image.thumbnail(size, Image.ANTIALIAS)\n",
    "    \n",
    "    # Create a new square background image\n",
    "    new_image = Image.new(\"RGB\", size, bg_color)\n",
    "    \n",
    "    # Paste the resized image into the center of the square background\n",
    "    new_image.paste(src_image, (int((size[0] - src_image.size[0]) / 2), int((size[1] - src_image.size[1]) / 2)))\n",
    "  \n",
    "    # return the resized image\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path):\n",
    "\n",
    "    # Load all the images\n",
    "    transformation = transforms.Compose([\n",
    "        # Randomly augment the image data\n",
    "            # Random horizontal flip\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "            # Random vertical flip\n",
    "        transforms.RandomVerticalFlip(0.3),\n",
    "        # transform to tensors\n",
    "        transforms.ToTensor(),\n",
    "        # Normalize the pixel values (in R, G, and B channels)\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    # Load all of the images, transforming them\n",
    "    full_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=transformation\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Split into training (70% and testing (30%) datasets)\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    \n",
    "    # use torch.utils.data.random_split for training/test split\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "   \n",
    "    # define a loader for the training data we can iterate through in 50-image batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=50,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # define a loader for the testing data we can iterate through in 50-image batches\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=50,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "        \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders ready to read C:/Users/admin/Desktop/Final Project/train\n"
     ]
    }
   ],
   "source": [
    "final_train_folder = 'C:/Users/admin/Desktop/Final Project/train'\n",
    "# final_train_folder = 'C:/Users/admin/Desktop/Final Project/final_train'\n",
    "train_loader, test_loader = load_dataset(final_train_folder)\n",
    "batch_size = train_loader.batch_size\n",
    "print(\"Data loaders ready to read\", final_train_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming images...\n",
      "processing folder Compost\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing folder Landfill\n",
      "processing folder Recycle\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "size = (128,128)\n",
    "# Create the output folder if it doesn't already exist\n",
    "if not os.path.exists(final_train_folder):\n",
    "    os.makedirs(final_train_folder)\n",
    "\n",
    "# Loop through each subfolder in the input folder\n",
    "print('Transforming images...')\n",
    "# print(os.walk(train_folder))\n",
    "for root, folders, files in os.walk(train_folder):\n",
    "    # print(root)\n",
    "    for folder in folders:\n",
    "        print('processing folder ' + folder)\n",
    "        # Create a matching subfolder in the output dir\n",
    "        saveFolder = os.path.join(final_train_folder,folder)\n",
    "        if not os.path.exists(saveFolder):\n",
    "            os.makedirs(saveFolder)\n",
    "        # Loop through the files in the subfolder\n",
    "        file_names = os.listdir(os.path.join(root,folder))\n",
    "        for file_name in file_names:\n",
    "            # Open the file\n",
    "            file_path = os.path.join(root,folder, file_name)\n",
    "            # print(\"reading \" + file_path)\n",
    "            image = Image.open(file_path)\n",
    "            # Create a resized version and save it\n",
    "            resized_image = resize_image(image, size)\n",
    "            saveAs = os.path.join(saveFolder, file_name)\n",
    "            # print(\"writing \" + saveAs)\n",
    "            resized_image.save(saveAs)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop): Dropout2d(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=24576, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Defining the Constructor\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # In the init function, we define each layer we will use in our model\n",
    "        \n",
    "        # Our images are RGB, so we have input channels = 3. \n",
    "        # We will apply 12 filters in the first convolutional layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # A second convolutional layer takes 12 input channels, and generates 24 outputs\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # We in the end apply max pooling with a kernel size of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # A drop layer deletes 20% of the features to help prevent overfitting\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        # Our 128x128 image tensors will be pooled twice with a kernel size of 2. 128/2/2 is 32.\n",
    "        # This means that our feature tensors are now 32 x 32, and we've generated 24 of them\n",
    "        \n",
    "        # We need to flatten these in order to feed them to a fully-connected layer\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # In the forward function, pass the data through the layers we defined in the init function\n",
    "        \n",
    "        # Use a ReLU activation function after layer 1 (convolution 1 and pool)\n",
    "        x = F.relu(self.pool(self.conv1(x))) \n",
    "        \n",
    "        # Use a ReLU activation function after layer 2\n",
    "        x = F.relu(self.pool(self.conv2(x)))  \n",
    "        \n",
    "        # Select some features to drop to prevent overfitting (only drop during training)\n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        # Feed to fully-connected layer to predict class\n",
    "        x = self.fc(x)\n",
    "        # Return class probabilities via a log_softmax function \n",
    "        return torch.log_softmax(x, dim=1)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        print(self.summary())\n",
    "    \n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
    "    device = \"cuda\"\n",
    "\n",
    "# Create an instance of the model class and allocate it to the device\n",
    "model = Net(num_classes=len(classes)).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    # Process the images in batches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Use the CPU or GPU as appropriate\n",
    "        # Recall that GPU is optimized for the operations we are dealing with\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Push the data forward through the model layers\n",
    "        output = model(data)\n",
    "        \n",
    "        # Get the loss\n",
    "        loss = loss_criteria(output, target)\n",
    "\n",
    "        # Keep a running total\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print metrics so we see some progress\n",
    "        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "            \n",
    "    # return average loss for the epoch\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in test_loader:\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Get the predicted classes for this batch\n",
    "            output = model(data)\n",
    "            \n",
    "            # Calculate the loss for this batch\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            \n",
    "            # Calculate the accuracy for this batch\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    # Calculate the average loss and total accuracy for this epoch\n",
    "    avg_loss = test_loss / batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    # return average loss for the epoch\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 12, 128, 128]             336\n",
      "         MaxPool2d-2           [-1, 12, 64, 64]               0\n",
      "            Conv2d-3           [-1, 24, 64, 64]           2,616\n",
      "         MaxPool2d-4           [-1, 24, 32, 32]               0\n",
      "         Dropout2d-5           [-1, 24, 32, 32]               0\n",
      "            Linear-6                    [-1, 3]          73,731\n",
      "================================================================\n",
      "Total params: 76,683\n",
      "Trainable params: 76,683\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 3.00\n",
      "Params size (MB): 0.29\n",
      "Estimated Total Size (MB): 3.48\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(3,128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 1 Loss: 1.031379\n",
      "\tTraining batch 2 Loss: 1.046224\n",
      "\tTraining batch 3 Loss: 1.058652\n",
      "\tTraining batch 4 Loss: 1.028192\n",
      "\tTraining batch 5 Loss: 1.058104\n",
      "Training set: Average loss: 1.044510\n",
      "Validation set: Average loss: 1.034444, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 1.022713\n",
      "\tTraining batch 2 Loss: 1.042925\n",
      "\tTraining batch 3 Loss: 1.057468\n",
      "\tTraining batch 4 Loss: 1.023323\n",
      "\tTraining batch 5 Loss: 1.057121\n",
      "Training set: Average loss: 1.040710\n",
      "Validation set: Average loss: 1.030940, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 1.017656\n",
      "\tTraining batch 2 Loss: 1.040577\n",
      "\tTraining batch 3 Loss: 1.056848\n",
      "\tTraining batch 4 Loss: 1.019237\n",
      "\tTraining batch 5 Loss: 1.056735\n",
      "Training set: Average loss: 1.038211\n",
      "Validation set: Average loss: 1.028092, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 1.014114\n",
      "\tTraining batch 2 Loss: 1.039053\n",
      "\tTraining batch 3 Loss: 1.056770\n",
      "\tTraining batch 4 Loss: 1.016015\n",
      "\tTraining batch 5 Loss: 1.056827\n",
      "Training set: Average loss: 1.036556\n",
      "Validation set: Average loss: 1.025939, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 1.011702\n",
      "\tTraining batch 2 Loss: 1.038173\n",
      "\tTraining batch 3 Loss: 1.057062\n",
      "\tTraining batch 4 Loss: 1.013581\n",
      "\tTraining batch 5 Loss: 1.057214\n",
      "Training set: Average loss: 1.035546\n",
      "Validation set: Average loss: 1.024386, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 1.010145\n",
      "\tTraining batch 2 Loss: 1.037747\n",
      "\tTraining batch 3 Loss: 1.057541\n",
      "\tTraining batch 4 Loss: 1.011808\n",
      "\tTraining batch 5 Loss: 1.057722\n",
      "Training set: Average loss: 1.034992\n",
      "Validation set: Average loss: 1.023303, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 1.009206\n",
      "\tTraining batch 2 Loss: 1.037597\n",
      "\tTraining batch 3 Loss: 1.058054\n",
      "\tTraining batch 4 Loss: 1.010563\n",
      "\tTraining batch 5 Loss: 1.058217\n",
      "Training set: Average loss: 1.034727\n",
      "Validation set: Average loss: 1.022573, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 1.008686\n",
      "\tTraining batch 2 Loss: 1.037590\n",
      "\tTraining batch 3 Loss: 1.058499\n",
      "\tTraining batch 4 Loss: 1.009724\n",
      "\tTraining batch 5 Loss: 1.058617\n",
      "Training set: Average loss: 1.034623\n",
      "Validation set: Average loss: 1.022095, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 1.008432\n",
      "\tTraining batch 2 Loss: 1.037635\n",
      "\tTraining batch 3 Loss: 1.058826\n",
      "\tTraining batch 4 Loss: 1.009190\n",
      "\tTraining batch 5 Loss: 1.058892\n",
      "Training set: Average loss: 1.034595\n",
      "Validation set: Average loss: 1.021799, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 1.008334\n",
      "\tTraining batch 2 Loss: 1.037683\n",
      "\tTraining batch 3 Loss: 1.059025\n",
      "\tTraining batch 4 Loss: 1.008881\n",
      "\tTraining batch 5 Loss: 1.059043\n",
      "Training set: Average loss: 1.034593\n",
      "Validation set: Average loss: 1.021631, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 11\n",
      "\tTraining batch 1 Loss: 1.008322\n",
      "\tTraining batch 2 Loss: 1.037713\n",
      "\tTraining batch 3 Loss: 1.059114\n",
      "\tTraining batch 4 Loss: 1.008734\n",
      "\tTraining batch 5 Loss: 1.059096\n",
      "Training set: Average loss: 1.034596\n",
      "Validation set: Average loss: 1.021555, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 12\n",
      "\tTraining batch 1 Loss: 1.008353\n",
      "\tTraining batch 2 Loss: 1.037724\n",
      "\tTraining batch 3 Loss: 1.059122\n",
      "\tTraining batch 4 Loss: 1.008701\n",
      "\tTraining batch 5 Loss: 1.059080\n",
      "Training set: Average loss: 1.034596\n",
      "Validation set: Average loss: 1.021543, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 13\n",
      "\tTraining batch 1 Loss: 1.008402\n",
      "\tTraining batch 2 Loss: 1.037719\n",
      "\tTraining batch 3 Loss: 1.059079\n",
      "\tTraining batch 4 Loss: 1.008741\n",
      "\tTraining batch 5 Loss: 1.059024\n",
      "Training set: Average loss: 1.034593\n",
      "Validation set: Average loss: 1.021572, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 14\n",
      "\tTraining batch 1 Loss: 1.008456\n",
      "\tTraining batch 2 Loss: 1.037706\n",
      "\tTraining batch 3 Loss: 1.059011\n",
      "\tTraining batch 4 Loss: 1.008822\n",
      "\tTraining batch 5 Loss: 1.058952\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021625, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 15\n",
      "\tTraining batch 1 Loss: 1.008506\n",
      "\tTraining batch 2 Loss: 1.037691\n",
      "\tTraining batch 3 Loss: 1.058937\n",
      "\tTraining batch 4 Loss: 1.008920\n",
      "\tTraining batch 5 Loss: 1.058878\n",
      "Training set: Average loss: 1.034586\n",
      "Validation set: Average loss: 1.021686, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 16\n",
      "\tTraining batch 1 Loss: 1.008550\n",
      "\tTraining batch 2 Loss: 1.037676\n",
      "\tTraining batch 3 Loss: 1.058867\n",
      "\tTraining batch 4 Loss: 1.009018\n",
      "\tTraining batch 5 Loss: 1.058813\n",
      "Training set: Average loss: 1.034585\n",
      "Validation set: Average loss: 1.021745, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 17\n",
      "\tTraining batch 1 Loss: 1.008584\n",
      "\tTraining batch 2 Loss: 1.037664\n",
      "\tTraining batch 3 Loss: 1.058810\n",
      "\tTraining batch 4 Loss: 1.009103\n",
      "\tTraining batch 5 Loss: 1.058761\n",
      "Training set: Average loss: 1.034584\n",
      "Validation set: Average loss: 1.021797, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 18\n",
      "\tTraining batch 1 Loss: 1.008610\n",
      "\tTraining batch 2 Loss: 1.037655\n",
      "\tTraining batch 3 Loss: 1.058765\n",
      "\tTraining batch 4 Loss: 1.009171\n",
      "\tTraining batch 5 Loss: 1.058722\n",
      "Training set: Average loss: 1.034585\n",
      "Validation set: Average loss: 1.021837, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 19\n",
      "\tTraining batch 1 Loss: 1.008627\n",
      "\tTraining batch 2 Loss: 1.037648\n",
      "\tTraining batch 3 Loss: 1.058735\n",
      "\tTraining batch 4 Loss: 1.009220\n",
      "\tTraining batch 5 Loss: 1.058696\n",
      "Training set: Average loss: 1.034585\n",
      "Validation set: Average loss: 1.021864, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 20\n",
      "\tTraining batch 1 Loss: 1.008637\n",
      "\tTraining batch 2 Loss: 1.037644\n",
      "\tTraining batch 3 Loss: 1.058716\n",
      "\tTraining batch 4 Loss: 1.009251\n",
      "\tTraining batch 5 Loss: 1.058681\n",
      "Training set: Average loss: 1.034586\n",
      "Validation set: Average loss: 1.021881, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 21\n",
      "\tTraining batch 1 Loss: 1.008642\n",
      "\tTraining batch 2 Loss: 1.037642\n",
      "\tTraining batch 3 Loss: 1.058706\n",
      "\tTraining batch 4 Loss: 1.009267\n",
      "\tTraining batch 5 Loss: 1.058674\n",
      "Training set: Average loss: 1.034586\n",
      "Validation set: Average loss: 1.021889, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 22\n",
      "\tTraining batch 1 Loss: 1.008644\n",
      "\tTraining batch 2 Loss: 1.037641\n",
      "\tTraining batch 3 Loss: 1.058703\n",
      "\tTraining batch 4 Loss: 1.009272\n",
      "\tTraining batch 5 Loss: 1.058673\n",
      "Training set: Average loss: 1.034587\n",
      "Validation set: Average loss: 1.021891, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 23\n",
      "\tTraining batch 1 Loss: 1.008643\n",
      "\tTraining batch 2 Loss: 1.037641\n",
      "\tTraining batch 3 Loss: 1.058704\n",
      "\tTraining batch 4 Loss: 1.009271\n",
      "\tTraining batch 5 Loss: 1.058676\n",
      "Training set: Average loss: 1.034587\n",
      "Validation set: Average loss: 1.021888, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 24\n",
      "\tTraining batch 1 Loss: 1.008641\n",
      "\tTraining batch 2 Loss: 1.037641\n",
      "\tTraining batch 3 Loss: 1.058708\n",
      "\tTraining batch 4 Loss: 1.009264\n",
      "\tTraining batch 5 Loss: 1.058680\n",
      "Training set: Average loss: 1.034587\n",
      "Validation set: Average loss: 1.021884, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 25\n",
      "\tTraining batch 1 Loss: 1.008638\n",
      "\tTraining batch 2 Loss: 1.037642\n",
      "\tTraining batch 3 Loss: 1.058713\n",
      "\tTraining batch 4 Loss: 1.009257\n",
      "\tTraining batch 5 Loss: 1.058685\n",
      "Training set: Average loss: 1.034587\n",
      "Validation set: Average loss: 1.021879, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 26\n",
      "\tTraining batch 1 Loss: 1.008636\n",
      "\tTraining batch 2 Loss: 1.037643\n",
      "\tTraining batch 3 Loss: 1.058718\n",
      "\tTraining batch 4 Loss: 1.009249\n",
      "\tTraining batch 5 Loss: 1.058690\n",
      "Training set: Average loss: 1.034587\n",
      "Validation set: Average loss: 1.021874, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 27\n",
      "\tTraining batch 1 Loss: 1.008633\n",
      "\tTraining batch 2 Loss: 1.037644\n",
      "\tTraining batch 3 Loss: 1.058722\n",
      "\tTraining batch 4 Loss: 1.009243\n",
      "\tTraining batch 5 Loss: 1.058693\n",
      "Training set: Average loss: 1.034587\n",
      "Validation set: Average loss: 1.021870, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 28\n",
      "\tTraining batch 1 Loss: 1.008631\n",
      "\tTraining batch 2 Loss: 1.037644\n",
      "\tTraining batch 3 Loss: 1.058725\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058696\n",
      "Training set: Average loss: 1.034587\n",
      "Validation set: Average loss: 1.021867, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 29\n",
      "\tTraining batch 1 Loss: 1.008630\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058728\n",
      "\tTraining batch 4 Loss: 1.009235\n",
      "\tTraining batch 5 Loss: 1.058698\n",
      "Training set: Average loss: 1.034587\n",
      "Validation set: Average loss: 1.021865, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 30\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058729\n",
      "\tTraining batch 4 Loss: 1.009233\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034587\n",
      "Validation set: Average loss: 1.021864, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 31\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009232\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034587\n",
      "Validation set: Average loss: 1.021863, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 32\n",
      "\tTraining batch 1 Loss: 1.008628\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009232\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034587\n",
      "Validation set: Average loss: 1.021863, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 33\n",
      "\tTraining batch 1 Loss: 1.008628\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009233\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034587\n",
      "Validation set: Average loss: 1.021864, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 34\n",
      "\tTraining batch 1 Loss: 1.008628\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009233\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034587\n",
      "Validation set: Average loss: 1.021864, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 35\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009234\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034587\n",
      "Validation set: Average loss: 1.021865, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 36\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009235\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034587\n",
      "Validation set: Average loss: 1.021865, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 37\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058729\n",
      "\tTraining batch 4 Loss: 1.009235\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 38\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058729\n",
      "\tTraining batch 4 Loss: 1.009236\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021865, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 39\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058729\n",
      "\tTraining batch 4 Loss: 1.009236\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 40\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058729\n",
      "\tTraining batch 4 Loss: 1.009236\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 41\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058729\n",
      "\tTraining batch 4 Loss: 1.009236\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 42\n",
      "\tTraining batch 1 Loss: 1.008630\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058729\n",
      "\tTraining batch 4 Loss: 1.009236\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 43\n",
      "\tTraining batch 1 Loss: 1.008630\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058729\n",
      "\tTraining batch 4 Loss: 1.009236\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 44\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058729\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 45\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058729\n",
      "\tTraining batch 4 Loss: 1.009236\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 46\n",
      "\tTraining batch 1 Loss: 1.008630\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 47\n",
      "\tTraining batch 1 Loss: 1.008630\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009236\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 48\n",
      "\tTraining batch 1 Loss: 1.008630\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 49\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 50\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 51\n",
      "\tTraining batch 1 Loss: 1.008630\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 52\n",
      "\tTraining batch 1 Loss: 1.008630\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 53\n",
      "\tTraining batch 1 Loss: 1.008630\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 54\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 55\n",
      "\tTraining batch 1 Loss: 1.008630\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 56\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 57\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 58\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 59\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 60\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 61\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 62\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 63\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 64\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 65\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009237\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 66\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 67\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 68\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 69\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 70\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 71\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 72\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 73\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 74\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 75\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 76\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 77\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 78\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 79\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 80\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 81\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058699\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 82\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 83\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 84\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 85\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 86\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 87\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058730\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034588\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 88\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 89\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 90\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 91\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 92\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 93\n",
      "\tTraining batch 1 Loss: 1.008630\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 94\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 95\n",
      "\tTraining batch 1 Loss: 1.008630\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 96\n",
      "\tTraining batch 1 Loss: 1.008630\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 97\n",
      "\tTraining batch 1 Loss: 1.008630\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 98\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 99\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009238\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n",
      "Epoch: 100\n",
      "\tTraining batch 1 Loss: 1.008629\n",
      "\tTraining batch 2 Loss: 1.037645\n",
      "\tTraining batch 3 Loss: 1.058731\n",
      "\tTraining batch 4 Loss: 1.009239\n",
      "\tTraining batch 5 Loss: 1.058700\n",
      "Training set: Average loss: 1.034589\n",
      "Validation set: Average loss: 1.021866, Accuracy: 39/97 (40%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use an \"Adam\" optimizer to adjust weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Specify the loss criteria\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "\n",
    "# Track metrics in these arrays\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "# Train over 10 epochs (We restrict to 10 for time issues)\n",
    "epochs = 100\n",
    "print('Training on', device)\n",
    "for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('C:/Users/admin/Desktop/GitHub/PyTorch/weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (drop): Dropout2d(p=0.2, inplace=False)\n",
       "  (fc): Linear(in_features=24576, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # Add a batch dimension\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5663, -3.3360, -0.9243])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# image_path = 'C:/Users/admin/Desktop/Final Project/val/Recycle/00000006.jpg'\n",
    "# image_path = 'C:/Users/admin/Desktop/GitHub/PyTorch/banana.jpg'\n",
    "image_path = \"C:/Users/admin/Desktop/Final Project/val/Recycle/metal14.jpg\"\n",
    "size = (128,128)\n",
    "image = Image.open(image_path)\n",
    "input_image = resize_image(image, size)\n",
    "# resized_image.save('C:/Users/admin/Desktop/GitHub/PyTorch/banana.jpg')\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "input_image = transform(input_image)\n",
    "input_image = input_image.unsqueeze(0)\n",
    "# Step 4: Perform Inference\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)\n",
    "    print(output[0])\n",
    "# Convert the output to probabilities using softmax\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "predicted_class = torch.argmax(probabilities).item()\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions from test set...\n"
     ]
    }
   ],
   "source": [
    "# Defining Labels and Predictions\n",
    "truelabels = []\n",
    "predictions = []\n",
    "model.eval()\n",
    "print(\"Getting predictions from test set...\")\n",
    "for data, target in test_loader:\n",
    "    for label in target.data.numpy():\n",
    "        truelabels.append(label)\n",
    "    for prediction in model(data).data.numpy().argmax(1):\n",
    "        predictions.append(prediction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(truelabels, predictions)\n",
    "tick_marks = np.arange(len(classes))\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = classes, columns = classes)\n",
    "plt.figure(figsize = (7,7))\n",
    "sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\n",
    "plt.xlabel(\"Predicted Shape\", fontsize = 20)\n",
    "plt.ylabel(\"True Shape\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
